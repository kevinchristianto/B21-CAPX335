{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rice Disease Prediction - MobileNetV2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9o-OTGahT3v"
      },
      "source": [
        "# Rice Disease Prediction - MobileNetV2\n",
        "\n",
        "This notebook consists of the process of training a **Deep Neural Network** by using **MobileNetV2** by Google architecture through **Transfer Learning**.\n",
        "\n",
        "The dataset that we use is from **Kaggle** by **Tedi Setiady** (https://www.kaggle.com/tedisetiady/leaf-rice-disease-indonesia). The dataset consists of 3 classes (**Blast, Blight, and Tungro**) with 80 images in each class.\n",
        "\n",
        "Even though the size is only 80 images for each class, but this dataset was taken in South Sulawesi, Indonesia so that we can expect this dataset will work better in Indonesia because the symptoms of the diseases may vary for each country.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQy_gA0CsKJY"
      },
      "source": [
        "> **Import and install all the required library**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jHAsuKdTve4"
      },
      "source": [
        "!pip -q install tensorflowjs\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzvFRvHEsYDi"
      },
      "source": [
        "\n",
        "\n",
        "> **Mount Google Drive to `/drive`**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb8Zkg4V_jTr"
      },
      "source": [
        "drive.mount('/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsQHlhr9sgh1"
      },
      "source": [
        "> **Extract the dataset from Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVBBvUDEZp0F"
      },
      "source": [
        "zip_loc = '/drive/My Drive/Datasets/dataset-tedi.zip'\n",
        "zip_ref = zipfile.ZipFile(zip_loc, 'r')\n",
        "zip_ref.extractall('/content/')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4IQlAz3sw8g"
      },
      "source": [
        "> **Prepare the ImageDataGenerator for each training, validation, and test set. And set some augmentation such as rotation range, zoom_range, flip, etc.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVFL4mWRT0k7"
      },
      "source": [
        "DATASET_DIR = 'Splitted/'\n",
        "TRAINING_DIR = os.path.join(DATASET_DIR, 'train')\n",
        "VALIDATION_DIR = os.path.join(DATASET_DIR, 'val')\n",
        "TEST_DIR = os.path.join(DATASET_DIR, 'test')\n",
        "TARGET_SIZE = (224, 224)\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale = 1/255,\n",
        "    rotation_range=90,\n",
        "    width_shift_range=.2,\n",
        "    height_shift_range=.2,\n",
        "    zoom_range=.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAINING_DIR,\n",
        "    target_size=TARGET_SIZE,\n",
        "    class_mode='categorical',\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(\n",
        "    rescale = 1/255,\n",
        "    rotation_range=90,\n",
        "    width_shift_range=.2,\n",
        "    height_shift_range=.2,\n",
        "    zoom_range=.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    VALIDATION_DIR,\n",
        "    target_size=TARGET_SIZE,\n",
        "    class_mode='categorical',\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale = 1/255,\n",
        "    rotation_range=90,\n",
        "    width_shift_range=.2,\n",
        "    height_shift_range=.2,\n",
        "    zoom_range=.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    TEST_DIR,\n",
        "    target_size=TARGET_SIZE,\n",
        "    class_mode='categorical',\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSU6HT6atLmi"
      },
      "source": [
        "> **Initialisation of MobileNetV2 with weights from ImageNet, and set each layer to non-trainable**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoCiZxhDCF_s"
      },
      "source": [
        "mobilenet = MobileNetV2(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "mobilenet.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWF1SnNntVCU"
      },
      "source": [
        "> **Declare our model by taking the output of the MobileNetV2 into our model and adding a GlobalAveragePooling2D and some Hidden layer (Dense layer) with ReLU activation for each Dense layer and Softmax for the output layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1gJLRciPIql"
      },
      "source": [
        "x = keras.layers.GlobalAveragePooling2D()(mobilenet.output)\n",
        "x = keras.layers.Dense(512, activation='relu')(x)\n",
        "x = keras.layers.Dropout(.2)(x)\n",
        "x = keras.layers.Dense(512, activation='relu')(x)\n",
        "x = keras.layers.Dropout(.2)(x)\n",
        "x = keras.layers.Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = keras.models.Model(mobilenet.input, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AWC3HY1uIen"
      },
      "source": [
        "> **Compile the model, and train for a few epoch to initialize the weights for the top layers so that we can continue to train some of top layers of MobileNetV2**\n",
        "\n",
        "We use RMSprop optimizer as of the MobileNetV2 was trained using the ImageNet dataset.\n",
        "\n",
        "Reference: https://openaccess.thecvf.com/content_cvpr_2018/papers/Sandler_MobileNetV2_Inverted_Residuals_CVPR_2018_paper.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHoD-PDwZJjB"
      },
      "source": [
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=tf.optimizers.RMSprop(learning_rate=0.045, momentum=.9),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=10\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH_QRZLCuhzh"
      },
      "source": [
        "> **Print each layer number with coresponding layer name, and plot the model overall architecture if necessary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k6zytLTzBo2"
      },
      "source": [
        "for i, layer in enumerate(model.layers):\n",
        "    print(i, layer.name, layer.trainable)\n",
        "\n",
        "# To plot the model, uncomment the line below\n",
        "keras.utils.plot_model(model, 'model_graph.png', show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_z5dR7Hu9uo"
      },
      "source": [
        "> **Set some top Convolutional layers to be trainable to fine-tune it's weights, and compile the model again. Change the number of layer to be trainable accordingly**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEXogN_I6gaX"
      },
      "source": [
        "for layer in model.layers[107:]:\n",
        "    layer.trainable=True\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.optimizers.RMSprop(learning_rate=1e-5, momentum=.9),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V6DpYjyvgJc"
      },
      "source": [
        "> **Train the model for 100 epochs and use EarlyStopping (for 15 patience) and ModelCheckpoint (save each epoch weights) as the callbacks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40Tu_EAVT2jH"
      },
      "source": [
        "weight_filepath = \"/drive/My Drive/Model Checkpoint/Rice Leaf Disease - MobileNetV2/epoch: {epoch:02d} - acc: {accuracy:.2f} - val_acc {val_accuracy:.2f}.hdf5\"\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=15,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "    weight_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=False,\n",
        "    mode='auto',\n",
        "    save_weights_only=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=100,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[early_stopping, checkpoint]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLmHbgZbv2MY"
      },
      "source": [
        "> **Plot the training result (accuracy and loss) for both training and validation values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X05fBETBT2qC"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He8Ec0M2wfTs"
      },
      "source": [
        "> **Save the model as Keras file format (.h5)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tc30MMKT74f"
      },
      "source": [
        "model.save('/drive/My Drive/Trained Model/rice_disease-tedi-mobilenet.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RR-KSbKwuYi"
      },
      "source": [
        "> **Convert the Keras model into TensorFlow.js model format. Here I set the weight_shard_size_bytes to 60000000 so the weight shard file will be only one. After converting the model to TensorFlow.js format, zip the folder of converted model then copy it to Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP3yUn4l10Ft"
      },
      "source": [
        "!tensorflowjs_converter --weight_shard_size_bytes 60000000 --input_format=keras tedi-mobilenet.h5 ./tedi-mobilenet\n",
        "\n",
        "path = 'tedi-mobilenet/'\n",
        "with zipfile.ZipFile('mobilenet-json.zip', 'w', zipfile.ZIP_DEFLATED) as zipObj:\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for f in files:\n",
        "            zipObj.write(os.path.join(root, f), os.path.relpath(os.path.join(root, f), os.path.join(path, '..')))\n",
        "\n",
        "!cp mobilenet-json.zip '/drive/My Drive/Trained Model/rice_disease-tedi-mobilenet-js.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeukMXPKxqa8"
      },
      "source": [
        "> **Convert the Keras model into TensorFlow Lite model format, then copy the converted model to Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQvUYh-vvVJi"
      },
      "source": [
        "model = tf.keras.models.load_model('tedi-mobilenet.h5')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "tflite_model_file = 'tedi-mobilenet.tflite'\n",
        "\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "!cp tedi-mobilenet.tflite '/drive/My Drive/Trained Model/rice_disease-tedi-mobilenet.tflite'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}